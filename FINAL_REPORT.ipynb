{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fcf0f8-4e1c-436a-a569-95bcab73f3bc",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "course:   DS 5001 \n",
    "module:   Final Project\n",
    "topic:    Exploratory Text Analytics using Television Sitcom Scripts Final Report\n",
    "author:   Eric Tria\n",
    "date:     5 May 2023\n",
    "```\n",
    "\n",
    "# Student Info\n",
    "\n",
    "```yaml\n",
    "name:     Eric Tria\n",
    "user_id:  emt4wf\n",
    "email:    emt4wf@virginia.edu\n",
    "```\n",
    "\n",
    "# Project: Exploratory Text Analytics using Television Sitcom Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfdf1d-03e5-400c-8ea1-31b199acdf86",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Television shows have been around for decades and have been part of the daily routine of a lot of people. One interesting thing to notice is how shows have somewhat changed over the years. For this project, I will be focusing on sitcoms or situational comedies. In earlier years, sitcoms were known for being filmed in front of a live audience where the laughter and reactions are recorded. More recently, however, single-camera comedies are becoming more popular. Single-camera comedies are filmed without a live audience where the scripts are more similar a [feature comedy](https://screencraft.org/blog/differences-single-camera-multi-camera-tv-pilot-scripts/). Due to these differences, I will be focusing on epsisode scripts of three popular single-camera sitcoms from the 2000s and 2010s: Parks and Recreation (2009-2015), Brooklyn Nine-Nine (2013-2021), and The Office US (2005-2013). I selected these three shows because they revolve around characters in various workspaces: a government office, police precinct, and corporate office. Given these three popular shows, are there similarities between their scripts in terms of the general topics, language used, sentiments, and such? To answer this question, I went through the scripts of all the episodes of these shows. \n",
    "\n",
    "The corpus used for this project contains episode scripts for all the seasons of each show: 7 seasons for Parks and Recreation, 8 for Brooklyn Nine-Nine, and 9 for The Office US. A sitcom episode usually runs between 20 to 30 minutes, so each episode script would be relatively short compared to a full movie. Sitcoms usually follow a three-act structure [source](https://www.masterclass.com/articles/how-to-write-a-sitcom) where each act consists of 3-5 scenes each [source](https://www.theatlantic.com/entertainment/archive/2014/12/cracking-the-sitcom-code/384068/). In total, this provided me with a large corpus to work with in order to extract insights that can possibly answer the initial question that I posed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19110d1-1569-410c-8a96-917dd186e4e3",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "\n",
    "### 2.1 Provenance\n",
    "\n",
    "The episode scripts were programmatically scraped from [Sublikescript.com](https://subslikescript.com/), which is a website that contains scripts of a wide range of shows. The website had the scripts for the shows that I wanted to use for my project. The links I used were:\n",
    "- [Parks and Recreation](https://subslikescript.com/series/Parks_and_Recreation-1266020)\n",
    "- [Brooklyn Nine-Nine](https://subslikescript.com/series/Brooklyn_Nine-Nine-2467372)\n",
    "- [The Office US](https://subslikescript.com/series/The_Office-386676) \n",
    "\n",
    "In order to do this, I wrote a Python class to do the web scraping which is located in `/lib/script_scraper.py`. This Python class takes in a Sublikescript link and converts it into a corpus of lines and tokens. \n",
    "\n",
    "In addition to the scripts, I extracted additional data from Wikipedia to add more data to each episode. I was able to eaxtract information such as episode title, director, writer, date, and the total US viewers in millions for each episode. For each season, I was also able to get the Rotten Tomato rating, which can show how critics view these shows. I also wrote a Python script to scrape the data from Wikipedia located in `/lib/wiki_scraper.py`, which takes a link and converts the details into a tabular format. This information was extracted from the following links:\n",
    "- Parks and Recreation: [Wikipedia page](https://en.wikipedia.org/wiki/Parks_and_Recreation) and [episode list](https://en.wikipedia.org/wiki/List_of_Parks_and_Recreation_episodes)\n",
    "- Brooklyn Nine-Nine: [Wikipedia page](https://en.wikipedia.org/wiki/Brooklyn_Nine-Nine) and [episode list](https://en.wikipedia.org/wiki/List_of_Brooklyn_Nine-Nine_episodes)\n",
    "- The Office US: [Wikipedia page](https://en.wikipedia.org/wiki/The_Office_(American_TV_series) and [episode list](https://en.wikipedia.org/wiki/List_of_The_Office_(American_TV_series)_episodes)\n",
    "\n",
    "The process of using these scripts to generate the corpus is available in a Jupyter notebook called `EXTRACT_DATA.ipynb`.\n",
    "\n",
    "### 2.2 Location\n",
    "\n",
    "The raw scraped data can be accessed through at the `/raw` folder of the repository. The different files are:\n",
    "- Parks and Recreation:\n",
    "    - `/raw/PNR-LINES.csv`\n",
    "    - `/raw/PNR-TOKENS.csv`\n",
    "    - `/raw/PNR-LIB.csv`\n",
    "    - `/raw/PNR-LIB-EPS.csv`\n",
    "- Brooklyn Nine-Nine:\n",
    "    - `/raw/B99-LINES.csv`\n",
    "    - `/raw/B99-TOKENS.csv`\n",
    "    - `/raw/B99-LIB.csv`\n",
    "    - `/raw/B99-LIB-EPS.csv`\n",
    "- The Office US:\n",
    "    - `/raw/OFFICE-LINES.csv`\n",
    "    - `/raw/OFFICE-TOKENS.csv`\n",
    "    - `/raw/OFFICE-LIB.csv`\n",
    "    - `/raw/OFFICE-LIB-EPS.csv`\n",
    "\n",
    "I have also provided files that combined all three series in the `/final` folder of the repository. The different files are:\n",
    "- `/final/ALL-CORPUS.csv`\n",
    "- `/final/ALL-LIB.csv`\n",
    "- `/final/ALL-LIB-EPS.csv`\n",
    "\n",
    "### 2.3 Description\n",
    "\n",
    "In general, the data would be split in the following: series, seasons, episodes, scenes, lines, and tokens. Since the data source did not have explicit scene tagging, I used the structure discusses earlier where each sitcom would have 3 acts with 5 scenes each. For this project, I used an arbitrary number of 15 scenes per episode.\n",
    "\n",
    "#### Files under `/raw`\n",
    "The raw version of the corpus called `LINES` contains lines for each script. This table is formatted as follows:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_id  | Index   | ID for the TV series |\n",
    "| season_id  | Index   | Season number        |\n",
    "| episode_id | Index   | Episode number       |\n",
    "| scene_id   | Index   | Scene number         |\n",
    "| line_id    | Index   | Line number          |\n",
    "| line       | Feature | Line contents        |\n",
    "\n",
    "The tokenized version of the corpus called `TOKENS` contains the script broken down into individual tokens. This table is formatted as follows:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_id  | Index   | ID for the TV series |\n",
    "| season_id  | Index   | Season number        |\n",
    "| episode_id | Index   | Episode number       |\n",
    "| scene_id   | Index   | Scene number         |\n",
    "| line_id    | Index   | Line number          |\n",
    "| token_id   | Index   | Token number         |\n",
    "| token_str  | Feature | String token         |\n",
    "| term_str   | Feature | Cleaned string term  |\n",
    "\n",
    "The `LIB` tables contain additional information on a season level:\n",
    "\n",
    "| Column           | Type    | Description                              |\n",
    "|------------------|---------|------------------------------------------|\n",
    "| series_id        | Index   | ID for the TV series                     |\n",
    "| season_id        | Index   | Season number                            |\n",
    "| num_episodes     | Feature | Number of episodes in the season         |\n",
    "| year             | Feature | TV season year of the season             |\n",
    "| viewers_millions | Feature | Average number of US viewers in millions |\n",
    "| rt_rating        | Feature | Rotten Tomato rating                     |\n",
    "| series_name      | Feature | Series name                              |\n",
    "\n",
    "The `LIB-EPS` tables contain additional information on an episode level:\n",
    "\n",
    "| Column        | Type    | Description                       |\n",
    "|---------------|---------|-----------------------------------|\n",
    "| series_id     | Index   | ID for the TV series              |\n",
    "| season_id     | Index   | Season number                     |\n",
    "| episode_id    | Index   | Episode number                    |\n",
    "| episode_title | Feature | Title of the episode              |\n",
    "| director      | Feature | Name of the episode's director    |\n",
    "| first_writer  | Feature | Name of the epsiode's main writer |\n",
    "| date          | Feature | Date when the episode aired       |\n",
    "| us_viewers    | Feature | Number of US viewers in millions  |\n",
    "\n",
    "#### Files under `/final`\n",
    "\n",
    "The `ALL-CORPUS.csv` file contains the same details as the raw `TOKENS` files but with additional features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| pos_tuple  | Feature   | Token and its tagged part of speech |\n",
    "| pos        | Feature   | Tagged part of speech       |\n",
    "\n",
    "The `ALL-LIB.csv` file contains the same details as the raw `LIB` files but with additional engineered features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_season  | Feature   | Combined label using series name and season number |\n",
    "| series_rotten_tomatoes        | Feature   | Category if rating is 90+ or not      |\n",
    "\n",
    "The `ALL-LIB-EPS.csv` file contains the same details as the raw `LIB-EPS` files but with additional engineered features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_season_ep  | Feature   | Combined label using series name, season number, and episode number |\n",
    "| year        | Feature   | Year when the episode aired      |\n",
    "\n",
    "In terms of sizes, the breakdown is as follows:\n",
    "- `ALL-CORPUS` has a total of **1,563,801** tokens\n",
    "- `ALL-LIB` has a total of **24** seasons\n",
    "- `ALL-LIB-EPS` has a total of **479** episodes\n",
    "\n",
    "### 2.4 Format\n",
    "\n",
    "All of the files listed above are in CSV format. The sources, however, were initially in HTML format since they were scraped from web pages. The HTML was transformed into text using BeautifulSoup's LXML parser and then that text was converted into the CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b1b90-c8d3-443b-b170-434ff86eec6c",
   "metadata": {},
   "source": [
    "# 3. Models\n",
    "\n",
    "Using this source data, I proceeded to create additional models to aid in my analysis of the corpus. These models and algorithms included the following: term frequency, hierarchical clustering, principal component analysis, topic modelling, word embeddings, and sentiment analysis.\n",
    "\n",
    "### 3.0 OHCO Explanation\n",
    "\n",
    "The OHCO or Ordered Hierarchy of Content Objects that I used for this corpus is:\n",
    "- Series\n",
    "- Seasons\n",
    "- Episodes\n",
    "- Scenes\n",
    "- Lines\n",
    "- Tokens\n",
    "\n",
    "For my analysis, I will treat this OHCO levels similarly to the usual OHCO levels for books:\n",
    "- Seasons used similarly to Books\n",
    "- Episodes used similarly to Chapters\n",
    "- Scenes used similarly to Paragraphs\n",
    "- Lines used similarly to Sentences\n",
    "\n",
    "### 3.1 VOCAB / Term Frequency\n",
    "\n",
    "The first model that I created is using term frequency and other metrics derived from it. This makes up the `VOCAB` table for my corpus. This table highlights the TFIDF or Term Frequency - Inverse Document Frequency, which is the frequency of a word in a document weighted by that word's information value over the corpus\n",
    ". The summary of the included fields are as follows:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "| term_str     | Index   | String term                                                 |\n",
    "| n            | Feature | Term frequency                                              |\n",
    "| n_chars      | Feature | Term length                                                 |\n",
    "| p            | Feature | Term probability                                            |\n",
    "| i            | Feature | Term information                                            |\n",
    "| h            | Feature | Term entropy                                                |\n",
    "| max_pos      | Feature | Most frequently associated part-of-speech character         |\n",
    "| n_pos        | Feature | POS ambiguity                                               |\n",
    "| cat_pos      | Feature | POS ambiguity                                               |\n",
    "| stop         | Feature | Dummy encoded variable if term is part of NLTK's stop words |\n",
    "| tfidf_mean   | Feature | Mean value of term TFIDF                                    |\n",
    "| tfidf_median | Feature | Median value of term TFIDF                                  |\n",
    "| tfidf_max    | Feature | Max value of term TFIDF                                     |\n",
    "| dfidf        | Feature | Document frequency                                          |\n",
    "\n",
    "For the whole corpus, TFIDF and DFIDF values were computed using **EPISODES** as the OHCO level due to memory constraints. This file is located at:\n",
    "- `/final/ALL-VOCAB-EPS.csv`\n",
    "\n",
    "Additionally, I created separate versions of the VOCAB table for each TV series with the TFIDF and DFIDF values computed using **SCENES** as the OHCO level. These files are located at:\n",
    "- `/final/PNR-VOCAB-SCENES.csv`\n",
    "- `/final/B99-VOCAB-SCENES.csv`\n",
    "- `/final/B99-VOCAB-SCENES.csv`\n",
    "\n",
    "I created this VOCAB tables using the functions from the `lib/corpus_enhancer.py` Python class and demonstrated in `EXTRACT_DATA.ipynb`.\n",
    "\n",
    "### 3.2 Hierarchical Clustering\n",
    "\n",
    "To create the hierarchical clustering model, I used the 1000 most significant terms in the VOCAB table using DFIDF as the significance measure. I also filtered the terms to those whose maximum part-of-speech belong to `NN NNS VB VBD VBG VBN VBP VBZ JJ JJR JJS RB RBR RBS`.\n",
    "\n",
    "I then created the three levels: Binary, Probabilistic, and Pythagorean/Euclidean. I then computed the distance using the following distance metrics: City block, Cosine, Euclidean, Jaccard, and Jensen-Shannon. I did the clustering to compate seasons and the final table is as follows:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "| doc_a     | Index   | First document formatted as (series, season)           |\n",
    "| doc_b            | Index | Second document formatted as (series, season) |\n",
    "| cityblock      | Feature | City block distance                                    |\n",
    "| cosine            | Feature | Cosine distance                                     |\n",
    "| euclidean            | Feature | Euclidean distance                                     |\n",
    "| jaccard            | Feature | Jaccard distance                                     |\n",
    "| js            | Feature | Jensen-Shannon distance                                     |\n",
    "\n",
    "This file is located at `/final/clustering/ALL-PAIRS.csv`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`\n",
    "\n",
    "### 3.3 Principal Component Analysis (PCA)\n",
    "\n",
    "The next model I created was for principal component analysis. For this model, I limited the VOCAB to nouns and plural nouns: `NN and NNS`. However, I noticed that in my corpus, some of the proper nouns were tagged as `NN`. To fix this, I did additional filtering on the VOCAB table to remove the names of the main and recurring characters of all 3 TV series. For the actual PCA model, I generated 10 components while also normalizing the input matrix. I also computed the principal components using episodes as the OHCO level due to memory constraints. There are two tables saved for PCA.\n",
    "\n",
    "The first one is a table of documents and components:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|PC0 to PC9 (10 columns)| Features | Associated values of the principal components|\n",
    "|LIB-EPS details (multiple columns)| Features | Additional details joined from the ALL-LIB-EPS table|\n",
    "\n",
    "This table is located at `/final/pca/ALL-DOC-COMPS.csv`\n",
    "\n",
    "The second table is a table of components and word counts (i.e., the “loadings”):\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|PC0 to PC9 (10 columns) | Features | Associated values of the principal components|\n",
    "|n|Feature| Word count|\n",
    "\n",
    "This table is located at `/final/pca/ALL-LOADINGS.csv`\n",
    "\n",
    "To conduct further exploration, I also generated separate principal components for each separate TV series. The files are also located at `/final/pca`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`\n",
    "\n",
    "### 3.4 Topic Models (LDA)\n",
    "\n",
    "For topic modelling, I also limited the VOCAB to nouns and plural nouns: `NN and NNS`. Similarly, I had to remove the names of main and recurring characters. I also noticed that there were some filler words that were not being tagged as stop words. To resolve this, I did additional filtering by removing the filler words before generating the topics. I used the following parameters for the topic model:\n",
    "- Ngram range: [1, 2]\n",
    "- Max features for the count vectorizer: 4000\n",
    "- Number of components for LDA: 20\n",
    "- Max iterations for LDA: 5\n",
    "- Number of words used to characterize a topic: 7\n",
    "\n",
    "After a few experiments in generating topics, I found that it is also effective to ignore terms that appear in more than 50% of the documents since a lot of the words in my corpus are usual conversational words. I did this by setting the *max_df* parameter of the CountVectorizer. I generated the topic models using scenes as the OHCO level. There are two tables saved for topic modelling.\n",
    "\n",
    "The first table is a table of document and topic concentrations:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|scene_id|Index|Scene number|\n",
    "|T00 to T19 (20 columns)|Features|Topic concentration values|\n",
    "\n",
    "This table is located at `/final/topic_modeling/ALL-DOCS.csv`\n",
    "\n",
    "The second table is a table of topics and term counts:\n",
    "\n",
    "**ADD HERE**\n",
    "\n",
    "### 3.5 Word Embeddings (word2vec)\n",
    "\n",
    "For the word embedding model, I limited the VOCAB to nouns and verbs: `'NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'`. Similar to PCA and LDA, I filtered out the names of the main and recurring characters. I used the following parameters for the word2vec model:\n",
    "- Window: 2\n",
    "- Vector size: 256\n",
    "- Minimum count: 50\n",
    "- Workers: 4\n",
    "\n",
    "The resulting table is a table of terms and embeddings:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|vector|Feature| word2vec vector|\n",
    "|x|Feature| x coordinate|\n",
    "|y|Feature| y coordinate|\n",
    "|n|Feature| Term count|\n",
    "|max_pos|Feature| Most frequent part-of-speech of the term|\n",
    "|pos_group|Feature| Either Noun or Verb|\n",
    "\n",
    "This table is located at `/final/word_embedding/ALL-COORDS.csv`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`. For additional exploration, I also generated separate word2vec models for each TV series which are also located at `/final/word_embedding`.\n",
    "\n",
    "### 3.6 Sentiment Analysis\n",
    "\n",
    "For sentiment analysis, I made use of the SALEX NRC lexicon. I applied this to my VOCAB table where sentiment values were then associated to the terms. These sentiment values were also multiplied by the term TFIDF values. I computed the sentiment analysis using episodes as the OHCO level. I was able to generate two tables for this model.\n",
    "\n",
    "The first table is a table of sentiment and emotion values as features of each term:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|anger|Feature| Anger value of the term|\n",
    "|anticipation|Feature| Anticipation value of the term|\n",
    "|disgust|Feature| Disgust value of the term|\n",
    "|fear|Feature| Fear value of the term|\n",
    "|joy|Feature| Joy value of the term|\n",
    "|sadness|Feature| Sadness value of the term|\n",
    "|surprise|Feature| Surprise value of the term|\n",
    "|trust|Feature| Trust value of the term|\n",
    "|sentiment|Feature| Sentiment value of the term|\n",
    "\n",
    "This table is located at `/final/sentiment_analysis/ALL-VOCAB-SA.csv`.\n",
    "\n",
    "The second table is a table of sentiment polarity and emotions for each document:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|anger|Feature| Anger value of the document|\n",
    "|anticipation|Feature| Anticipation value of the document|\n",
    "|disgust|Feature| Disgust value of the document|\n",
    "|fear|Feature| Fear value of the document|\n",
    "|joy|Feature| Joy value of the document|\n",
    "|sadness|Feature| Sadness value of the document|\n",
    "|surprise|Feature| Surprise value of the document|\n",
    "|trust|Feature| Trust value of the document|\n",
    "|sentiment|Feature| Sentiment value of the document|\n",
    "\n",
    "This table is located at `/final/sentiment_analysis/ALL-DOC-SA.csv`. This process is demonstrated at `TEXT_ANALYSIS.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09f40d-9afe-4dd7-9a9e-6126413d3534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612f790-554e-4fee-b427-1667b0767ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

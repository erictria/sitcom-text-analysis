{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fcf0f8-4e1c-436a-a569-95bcab73f3bc",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "course:   DS 5001 \n",
    "module:   Final Project\n",
    "topic:    Exploratory Text Analytics using Television Sitcom Scripts Final Report\n",
    "author:   Eric Tria\n",
    "date:     5 May 2023\n",
    "```\n",
    "\n",
    "# Student Info\n",
    "\n",
    "```yaml\n",
    "name:     Eric Tria\n",
    "user_id:  emt4wf\n",
    "email:    emt4wf@virginia.edu\n",
    "```\n",
    "\n",
    "# Project: Exploratory Text Analytics using Television Sitcom Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfdf1d-03e5-400c-8ea1-31b199acdf86",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Television shows have been around for decades and have been part of the daily routine of a lot of people. One interesting thing to notice is how shows have somewhat changed over the years. For this project, I will be focusing on sitcoms or situational comedies. In earlier years, sitcoms were known for being filmed in front of a live audience where the laughter and reactions are recorded. More recently, however, single-camera comedies are becoming more popular. Single-camera comedies are filmed without a live audience where the scripts are more similar to a feature comedy [[3]](https://screencraft.org/blog/differences-single-camera-multi-camera-tv-pilot-scripts/). Due to these differences, I will be focusing on epsisode scripts of three popular single-camera sitcoms from the 2000s and 2010s: Parks and Recreation (2009-2015), Brooklyn Nine-Nine (2013-2021), and The Office US (2005-2013). I selected these three shows because they revolve around characters in various workspaces: a government office, police precinct, and corporate office. Given these three popular shows, are there similarities between their scripts in terms of the general topics, language used, sentiments, and such? To answer this question, I went through the scripts of all the episodes of these shows. \n",
    "\n",
    "The corpus used for this project contains episode scripts for all the seasons of each show: 7 seasons for Parks and Recreation, 8 for Brooklyn Nine-Nine, and 9 for The Office US. A sitcom episode usually runs between 20 to 30 minutes, so each episode script would be relatively short compared to a full movie. Sitcoms usually follow a three-act structure [[2]](https://www.masterclass.com/articles/how-to-write-a-sitcom) where each act consists of 3-5 scenes each [[1]](https://www.theatlantic.com/entertainment/archive/2014/12/cracking-the-sitcom-code/384068/). In total, this provided me with a large corpus to work with in order to extract insights that can possibly answer the initial question that I posed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19110d1-1569-410c-8a96-917dd186e4e3",
   "metadata": {},
   "source": [
    "# 2. Data\n",
    "\n",
    "### 2.1 Provenance\n",
    "\n",
    "The episode scripts were programmatically scraped from [Sublikescript.com](https://subslikescript.com/), which is a website that contains scripts of a wide range of shows. The website had the scripts for the shows that I wanted to use for my project. The links I used were:\n",
    "- [Parks and Recreation](https://subslikescript.com/series/Parks_and_Recreation-1266020)\n",
    "- [Brooklyn Nine-Nine](https://subslikescript.com/series/Brooklyn_Nine-Nine-2467372)\n",
    "- [The Office US](https://subslikescript.com/series/The_Office-386676) \n",
    "\n",
    "In order to do this, I wrote a Python class to do the web scraping which is located in `/lib/script_scraper.py`. This Python class takes in a Sublikescript link and converts it into a corpus of lines and tokens. \n",
    "\n",
    "In addition to the scripts, I extracted additional data from Wikipedia to add more data to each episode. I was able to eaxtract information such as episode title, director, writer, date, and the total US viewers in millions for each episode. For each season, I was also able to get the Rotten Tomato rating, which can show how critics view these shows. I also wrote a Python script to scrape the data from Wikipedia located in `/lib/wiki_scraper.py`, which takes a link and converts the details into a tabular format. This information was extracted from the following links:\n",
    "- Parks and Recreation: [Wikipedia page](https://en.wikipedia.org/wiki/Parks_and_Recreation) and [episode list](https://en.wikipedia.org/wiki/List_of_Parks_and_Recreation_episodes)\n",
    "- Brooklyn Nine-Nine: [Wikipedia page](https://en.wikipedia.org/wiki/Brooklyn_Nine-Nine) and [episode list](https://en.wikipedia.org/wiki/List_of_Brooklyn_Nine-Nine_episodes)\n",
    "- The Office US: [Wikipedia page](https://en.wikipedia.org/wiki/The_Office_(American_TV_series) and [episode list](https://en.wikipedia.org/wiki/List_of_The_Office_(American_TV_series)_episodes)\n",
    "\n",
    "The process of using these scripts to generate the corpus is available in a Jupyter notebook called `EXTRACT_DATA.ipynb`.\n",
    "\n",
    "### 2.2 Location\n",
    "\n",
    "The raw scraped data can be accessed through at the `/raw` folder of the repository. The different files are:\n",
    "- Parks and Recreation:\n",
    "    - `/raw/PNR-LINES.csv`\n",
    "    - `/raw/PNR-TOKENS.csv`\n",
    "    - `/raw/PNR-LIB.csv`\n",
    "    - `/raw/PNR-LIB-EPS.csv`\n",
    "- Brooklyn Nine-Nine:\n",
    "    - `/raw/B99-LINES.csv`\n",
    "    - `/raw/B99-TOKENS.csv`\n",
    "    - `/raw/B99-LIB.csv`\n",
    "    - `/raw/B99-LIB-EPS.csv`\n",
    "- The Office US:\n",
    "    - `/raw/OFFICE-LINES.csv`\n",
    "    - `/raw/OFFICE-TOKENS.csv`\n",
    "    - `/raw/OFFICE-LIB.csv`\n",
    "    - `/raw/OFFICE-LIB-EPS.csv`\n",
    "\n",
    "I have also provided files that combined all three series in the `/final` folder of the repository. The different files are:\n",
    "- `/final/ALL-CORPUS.csv`\n",
    "- `/final/ALL-LIB.csv`\n",
    "- `/final/ALL-LIB-EPS.csv`\n",
    "\n",
    "### 2.3 Description\n",
    "\n",
    "In general, the data would be split in the following: series, seasons, episodes, scenes, lines, and tokens. Since the data source did not have explicit scene tagging, I used the structure discusses earlier where each sitcom would have 3 acts with 5 scenes each. For this project, I used an arbitrary number of 15 scenes per episode.\n",
    "\n",
    "#### Files under `/raw`\n",
    "The raw version of the corpus called `LINES` contains lines for each script. This table is formatted as follows:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_id  | Index   | ID for the TV series |\n",
    "| season_id  | Index   | Season number        |\n",
    "| episode_id | Index   | Episode number       |\n",
    "| scene_id   | Index   | Scene number         |\n",
    "| line_id    | Index   | Line number          |\n",
    "| line       | Feature | Line contents        |\n",
    "\n",
    "The tokenized version of the corpus called `TOKENS` contains the script broken down into individual tokens. This table is formatted as follows:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_id  | Index   | ID for the TV series |\n",
    "| season_id  | Index   | Season number        |\n",
    "| episode_id | Index   | Episode number       |\n",
    "| scene_id   | Index   | Scene number         |\n",
    "| line_id    | Index   | Line number          |\n",
    "| token_id   | Index   | Token number         |\n",
    "| token_str  | Feature | String token         |\n",
    "| term_str   | Feature | Cleaned string term  |\n",
    "\n",
    "The `LIB` tables contain additional information on a season level:\n",
    "\n",
    "| Column           | Type    | Description                              |\n",
    "|------------------|---------|------------------------------------------|\n",
    "| series_id        | Index   | ID for the TV series                     |\n",
    "| season_id        | Index   | Season number                            |\n",
    "| num_episodes     | Feature | Number of episodes in the season         |\n",
    "| year             | Feature | TV season year of the season             |\n",
    "| viewers_millions | Feature | Average number of US viewers in millions |\n",
    "| rt_rating        | Feature | Rotten Tomato rating                     |\n",
    "| series_name      | Feature | Series name                              |\n",
    "\n",
    "The `LIB-EPS` tables contain additional information on an episode level:\n",
    "\n",
    "| Column        | Type    | Description                       |\n",
    "|---------------|---------|-----------------------------------|\n",
    "| series_id     | Index   | ID for the TV series              |\n",
    "| season_id     | Index   | Season number                     |\n",
    "| episode_id    | Index   | Episode number                    |\n",
    "| episode_title | Feature | Title of the episode              |\n",
    "| director      | Feature | Name of the episode's director    |\n",
    "| first_writer  | Feature | Name of the epsiode's main writer |\n",
    "| date          | Feature | Date when the episode aired       |\n",
    "| us_viewers    | Feature | Number of US viewers in millions  |\n",
    "\n",
    "#### Files under `/final`\n",
    "\n",
    "The `ALL-CORPUS.csv` file contains the same details as the raw `TOKENS` files but with additional features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| pos_tuple  | Feature   | Token and its tagged part of speech |\n",
    "| pos        | Feature   | Tagged part of speech       |\n",
    "\n",
    "The `ALL-LIB.csv` file contains the same details as the raw `LIB` files but with additional engineered features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_season  | Feature   | Combined label using series name and season number |\n",
    "| series_rotten_tomatoes        | Feature   | Category if rating is 90+ or not      |\n",
    "\n",
    "The `ALL-LIB-EPS.csv` file contains the same details as the raw `LIB-EPS` files but with additional engineered features:\n",
    "\n",
    "| Column     | Type    | Description          |\n",
    "|------------|---------|----------------------|\n",
    "| series_season_ep  | Feature   | Combined label using series name, season number, and episode number |\n",
    "| year        | Feature   | Year when the episode aired      |\n",
    "\n",
    "In terms of sizes, the breakdown is as follows:\n",
    "- `ALL-CORPUS` has a total of **1,563,801** tokens\n",
    "- `ALL-LIB` has a total of **24** seasons\n",
    "- `ALL-LIB-EPS` has a total of **479** episodes\n",
    "\n",
    "### 2.4 Format\n",
    "\n",
    "All of the files listed above are in CSV format. The sources, however, were initially in HTML format since they were scraped from web pages. The HTML was transformed into text using BeautifulSoup's LXML parser and then that text was converted into the CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b1b90-c8d3-443b-b170-434ff86eec6c",
   "metadata": {},
   "source": [
    "# 3. Models\n",
    "\n",
    "Using this source data, I proceeded to create additional models to aid in my analysis of the corpus. These models and algorithms included the following: term frequency, hierarchical clustering, principal component analysis, topic modelling, word embeddings, and sentiment analysis.\n",
    "\n",
    "### 3.0 OHCO Explanation\n",
    "\n",
    "The OHCO or Ordered Hierarchy of Content Objects that I used for this corpus is:\n",
    "- Series\n",
    "- Seasons\n",
    "- Episodes\n",
    "- Scenes\n",
    "- Lines\n",
    "- Tokens\n",
    "\n",
    "For my analysis, I will treat this OHCO levels similarly to the usual OHCO levels for books:\n",
    "- Seasons used similarly to Books\n",
    "- Episodes used similarly to Chapters\n",
    "- Scenes used similarly to Paragraphs\n",
    "- Lines used similarly to Sentences\n",
    "\n",
    "### 3.1 VOCAB / Term Frequency\n",
    "\n",
    "The first model that I created is using term frequency and other metrics derived from it. This makes up the `VOCAB` table for my corpus. This table highlights the TFIDF or Term Frequency - Inverse Document Frequency, which is the frequency of a word in a document weighted by that word's information value over the corpus\n",
    ". The summary of the included fields are as follows:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "| term_str     | Index   | String term                                                 |\n",
    "| n            | Feature | Term frequency                                              |\n",
    "| n_chars      | Feature | Term length                                                 |\n",
    "| p            | Feature | Term probability                                            |\n",
    "| i            | Feature | Term information                                            |\n",
    "| h            | Feature | Term entropy                                                |\n",
    "| max_pos      | Feature | Most frequently associated part-of-speech character         |\n",
    "| n_pos        | Feature | POS ambiguity                                               |\n",
    "| cat_pos      | Feature | POS ambiguity                                               |\n",
    "| stop         | Feature | Dummy encoded variable if term is part of NLTK's stop words |\n",
    "| tfidf_mean   | Feature | Mean value of term TFIDF                                    |\n",
    "| tfidf_median | Feature | Median value of term TFIDF                                  |\n",
    "| tfidf_max    | Feature | Max value of term TFIDF                                     |\n",
    "| dfidf        | Feature | Document frequency                                          |\n",
    "\n",
    "For the whole corpus, TFIDF and DFIDF values were computed using **EPISODES** as the OHCO level due to memory constraints. This file is located at:\n",
    "- `/final/ALL-VOCAB-EPS.csv`\n",
    "\n",
    "Additionally, I created separate versions of the VOCAB table for each TV series with the TFIDF and DFIDF values computed using **SCENES** as the OHCO level. These files are located at:\n",
    "- `/final/PNR-VOCAB-SCENES.csv`\n",
    "- `/final/B99-VOCAB-SCENES.csv`\n",
    "- `/final/B99-VOCAB-SCENES.csv`\n",
    "\n",
    "I created this VOCAB tables using the functions from the `lib/corpus_enhancer.py` Python class and demonstrated in `EXTRACT_DATA.ipynb`.\n",
    "\n",
    "### 3.2 Hierarchical Clustering\n",
    "\n",
    "To create the hierarchical clustering model, I used the 1000 most significant terms in the VOCAB table using DFIDF as the significance measure. I also filtered the terms to those whose maximum part-of-speech belong to `NN NNS VB VBD VBG VBN VBP VBZ JJ JJR JJS RB RBR RBS`.\n",
    "\n",
    "I then created the three levels: Binary, Probabilistic, and Pythagorean/Euclidean. I then computed the distance using the following distance metrics: City block, Cosine, Euclidean, Jaccard, and Jensen-Shannon. I did the clustering to compate seasons and the final table is as follows:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "| doc_a     | Index   | First document formatted as (series, season)           |\n",
    "| doc_b            | Index | Second document formatted as (series, season) |\n",
    "| cityblock      | Feature | City block distance                                    |\n",
    "| cosine            | Feature | Cosine distance                                     |\n",
    "| euclidean            | Feature | Euclidean distance                                     |\n",
    "| jaccard            | Feature | Jaccard distance                                     |\n",
    "| js            | Feature | Jensen-Shannon distance                                     |\n",
    "\n",
    "This file is located at `/final/clustering/ALL-PAIRS.csv`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`\n",
    "\n",
    "### 3.3 Principal Component Analysis (PCA)\n",
    "\n",
    "The next model I created was for principal component analysis. For this model, I limited the VOCAB to nouns and plural nouns: `NN and NNS`. However, I noticed that in my corpus, some of the proper nouns were tagged as `NN`. To fix this, I did additional filtering on the VOCAB table to remove the names of the main and recurring characters of all 3 TV series. For the actual PCA model, I generated 10 components while also normalizing the input matrix. I also computed the principal components using episodes as the OHCO level due to memory constraints. There are two tables saved for PCA.\n",
    "\n",
    "The first one is a table of documents and components:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|PC0 to PC9 (10 columns)| Features | Associated values of the principal components|\n",
    "|LIB-EPS details (multiple columns)| Features | Additional details joined from the ALL-LIB-EPS table|\n",
    "\n",
    "This table is located at `/final/pca/ALL-DOC-COMPS.csv`\n",
    "\n",
    "The second table is a table of components and word counts (i.e., the “loadings”):\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|PC0 to PC9 (10 columns) | Features | Associated values of the principal components|\n",
    "|n|Feature| Word count|\n",
    "\n",
    "This table is located at `/final/pca/ALL-LOADINGS.csv`\n",
    "\n",
    "To conduct further exploration, I also generated separate principal components for each separate TV series. The files are also located at `/final/pca`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`\n",
    "\n",
    "### 3.4 Topic Models (LDA)\n",
    "\n",
    "For topic modelling, I also limited the VOCAB to nouns and plural nouns: `NN and NNS`. Similarly, I had to remove the names of main and recurring characters. I also noticed that there were some filler words that were not being tagged as stop words. To resolve this, I did additional filtering by removing the filler words before generating the topics. I used the following parameters for the topic model:\n",
    "- Ngram range: [1, 2]\n",
    "- Max features for the count vectorizer: 4000\n",
    "- Number of components for LDA: 20\n",
    "- Max iterations for LDA: 5\n",
    "- Number of words used to characterize a topic: 7\n",
    "\n",
    "After a few experiments in generating topics, I found that it is also effective to ignore terms that appear in more than 50% of the documents since a lot of the words in my corpus are usual conversational words. I did this by setting the *max_df* parameter of the CountVectorizer. I generated the topic models using scenes as the OHCO level. There are two tables saved for topic modelling.\n",
    "\n",
    "The first table is a table of document and topic concentrations:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|scene_id|Index|Scene number|\n",
    "|T00 to T19 (20 columns)|Features|Topic concentration values|\n",
    "\n",
    "This table is located at `/final/topic_modeling/ALL-DOCS.csv`\n",
    "\n",
    "The second table is a table of topics and term counts:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|T00 to T19 (20 columns)|Features| Topic concentration values|\n",
    "|n|Feature|Term count|\n",
    "\n",
    "This table is located at `/final/topic_modeling/ALL-VOCAB.csv`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`. For additional exploration, I also generated topic models using plural nouns only `NNS`. The resulting tables are also located at `final/topic_modeling`.\n",
    "\n",
    "### 3.5 Word Embeddings (word2vec)\n",
    "\n",
    "For the word embedding model, I limited the VOCAB to nouns and verbs: `'NN', 'NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'`. Similar to PCA and LDA, I filtered out the names of the main and recurring characters. I used the following parameters for the word2vec model:\n",
    "- Window: 2\n",
    "- Vector size: 256\n",
    "- Minimum count: 50\n",
    "- Workers: 4\n",
    "\n",
    "The resulting table is a table of terms and embeddings:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|vector|Feature| word2vec vector|\n",
    "|x|Feature| x coordinate|\n",
    "|y|Feature| y coordinate|\n",
    "|n|Feature| Term count|\n",
    "|max_pos|Feature| Most frequent part-of-speech of the term|\n",
    "|pos_group|Feature| Either Noun or Verb|\n",
    "\n",
    "This table is located at `/final/word_embedding/ALL-COORDS.csv`. This process is demonstrated in `TEXT_ANALYSIS.ipynb`. For additional exploration, I also generated separate word2vec models for each TV series which are also located at `/final/word_embedding`.\n",
    "\n",
    "### 3.6 Sentiment Analysis\n",
    "\n",
    "For sentiment analysis, I made use of the SALEX NRC lexicon. I applied this to my VOCAB table where sentiment values were then associated to the terms. These sentiment values were also multiplied by the term TFIDF values. I computed the sentiment analysis using episodes as the OHCO level. I was able to generate two tables for this model.\n",
    "\n",
    "The first table is a table of sentiment and emotion values as features of each term:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|term_str|Index| String term|\n",
    "|anger|Feature| Anger value of the term|\n",
    "|anticipation|Feature| Anticipation value of the term|\n",
    "|disgust|Feature| Disgust value of the term|\n",
    "|fear|Feature| Fear value of the term|\n",
    "|joy|Feature| Joy value of the term|\n",
    "|sadness|Feature| Sadness value of the term|\n",
    "|surprise|Feature| Surprise value of the term|\n",
    "|trust|Feature| Trust value of the term|\n",
    "|sentiment|Feature| Sentiment value of the term|\n",
    "\n",
    "This table is located at `/final/sentiment_analysis/ALL-VOCAB-SA.csv`.\n",
    "\n",
    "The second table is a table of sentiment polarity and emotions for each document:\n",
    "\n",
    "| Column       | Type    | Description                                                 |\n",
    "|--------------|---------|-------------------------------------------------------------|\n",
    "|series_id|Index|ID of the series|\n",
    "|season_id|Index| Season number|\n",
    "|episode_id|Index|Episode number|\n",
    "|anger|Feature| Anger value of the document|\n",
    "|anticipation|Feature| Anticipation value of the document|\n",
    "|disgust|Feature| Disgust value of the document|\n",
    "|fear|Feature| Fear value of the document|\n",
    "|joy|Feature| Joy value of the document|\n",
    "|sadness|Feature| Sadness value of the document|\n",
    "|surprise|Feature| Surprise value of the document|\n",
    "|trust|Feature| Trust value of the document|\n",
    "|sentiment|Feature| Sentiment value of the document|\n",
    "\n",
    "This table is located at `/final/sentiment_analysis/ALL-DOC-SA.csv`. This process is demonstrated at `TEXT_ANALYSIS.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887e02d-2a5e-48ca-93b4-18eea6def500",
   "metadata": {},
   "source": [
    "# 4. Exploration\n",
    "\n",
    "The entire code for the explorations that I did is `TEXT_ANALYSIS.ipynb`\n",
    "\n",
    "The first step I did for my exploration was to compute TFIDF values for the entire corpus. Due to memory constraints, I computed the TFIDF at the episode level of the OHCO. Additionally, I computed the TFIDF values separately for each TV series using scenes as the OHCO level, and that was able to run. This was the precedent to the other explorations I did.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "One of the initial things that I did was to check the similarity of different seasons from these shows and if there were similar seasons across different shows. I did this using hierarchical clustering and various distance metrics: City block, Cosine, Euclidean, Jaccard, and Jensen-Shannon. I was able to visualize the results using correlation tables and hierarchical cluster diagrams. The correlation table looks like this:\n",
    "\n",
    "![Correlation](pictures/clustering_1.png)\n",
    "\n",
    "I also plotted the hierarchical cluster diagrams for all the different metrics.\n",
    "\n",
    "City block:\n",
    "\n",
    "![City Block](pictures/clustering_2.png)\n",
    "\n",
    "Cosine:\n",
    "\n",
    "![Cosine](pictures/clustering_3.png)\n",
    "\n",
    "Euclidean:\n",
    "\n",
    "![Euclidean](pictures/clustering_4.png)\n",
    "\n",
    "Jaccard:\n",
    "\n",
    "![Jaccard](pictures/clustering_5.png)\n",
    "\n",
    "Jensen-Shannon:\n",
    "\n",
    "![Jensen-Shannon](pictures/clustering_2.png)\n",
    "\n",
    "Most of the distance metrics, except for Jaccard, showed similar results, with seasons from each series typically being connected closer to each other. It was interesting to see that Parks and Recreation season 7 and Brooklyn Nine-Nine Season 1 appeared to be similar in most of the distance metrics. One possible hypothesis is because of similarity of writing. Side note: Michael Schur wrote (or co-wrote) 2 episodes for both [Park and Recreation season 7](https://en.wikipedia.org/wiki/Parks_and_Recreation_(season_7)) and [Brooklyn Nine-Nine Season 1](https://en.wikipedia.org/wiki/Brooklyn_Nine-Nine_(season_1))! \n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "For PCA, I created a first model that only uses Nouns and Plural Nouns (NN and NNS). The significant parameters that I used were:\n",
    "- k = 10\n",
    "- norm_docs = True\n",
    "- center_by_mean = False\n",
    "- center_by_variance = False\n",
    "- OHCO = EPISODES\n",
    "\n",
    "I visualized this using scatter plots. It was the first and second principal components that showed distinct boundaries for each series. The first principal component looks like this:\n",
    "\n",
    "![1st PCA](pictures/pca_1.png)\n",
    "\n",
    "The second principal component looks like this:\n",
    "\n",
    "![2nd PCA](pictures/pca_2.png)\n",
    "\n",
    "I also visualized the loadings for the second principal component:\n",
    "\n",
    "![2nd PCA LOADINGS](pictures/pca_3.png)\n",
    "\n",
    "We can see that the different corners have distinct words that are more related to each individual show. In one corner there are words like `detective` (Brooklyn Nine-Nine), then in another corner it has `parks and government` (Parks and Recreation), and on the other it has `paper, company, sales` (The Office US)\n",
    "\n",
    "I also checked to see if the year of an episode showed any significance in the PCAs. I saw in the first principal component that there were some noticeable patterns but not as easilty distinguishable:\n",
    "\n",
    "![1st PCA - Years](pictures/pca_4.png)\n",
    "\n",
    "![1st PCA - Years, LOADINGS](pictures/pca_5.png)\n",
    "\n",
    "I also tried to check other labels if they showed any distinct groups but they weren't noticeable.\n",
    "\n",
    "Seasons (2nd PC):\n",
    "\n",
    "![2nd PC - Seasons](pictures/pca_6.png)\n",
    "\n",
    "Rotten Tomato Rating (1st PC):\n",
    "\n",
    "![1st PC - Rotten Tomatoes](pictures/pca_7.png)\n",
    "\n",
    "Director (1st PC):\n",
    "\n",
    "![1st PC - Director](pictures/pca_8.png)\n",
    "\n",
    "US Viewers (1st PC):\n",
    "\n",
    "![2nd PC - US Viewers](pictures/pca_9.png)\n",
    "\n",
    "I also tried to check if the boundaries for directors would show more in separate principal components for each series, but they just ended up the same.\n",
    "\n",
    "### Topic Modeling (LDA)\n",
    "\n",
    "For topic modeling I used the following important parameters:\n",
    "- ngram_range = [1,2]\n",
    "- n_terms = 4000\n",
    "- n_topics = 20\n",
    "- max_iter = 5\n",
    "- n_top_terms = 7\n",
    "- max_df = 0.5\n",
    "- OHCO = SCENES\n",
    "\n",
    "I also removed additional filler words that I observed were popping up consistently when I was initially generating the models. \n",
    "\n",
    "The first model I created was for nouns (NN and NNS) which created 20 topics in total. Here is a look of the topic distributions for some documents:\n",
    "\n",
    "![Nouns Model](pictures/topic_1.png)\n",
    "\n",
    "I was also able to check which topics were more associated with which TV series:\n",
    "\n",
    "![Nouns Model - Association](pictures/topic_2.png)\n",
    "\n",
    "It wasn't surprising to see the top topics for each of the TV series:\n",
    "- Parks and Recreation: `T14 time, wait, look, god, need, mean, way`\n",
    "- Brooklyn Nine-Nine: `T17 need, good, mean, tell, look, work, thank`\n",
    "- The Office US: `T10 good, time, work, need, party, thank, way`\n",
    "\n",
    "Most of the top topics had more generic words. This pushed me to generate a second model for my analysis. I generated a second topic model using the same parameters but with plural nouns (NNS) only:\n",
    "\n",
    "![Plural Nouns Model](pictures/topic_3.png)\n",
    "\n",
    "![Plural Nouns Model - Association](pictures/topic_4.png)\n",
    "\n",
    "For this model, we were able to see more distinct words in the top topics for each of the TV series:\n",
    "- Parks and Recreation: `T11 parks, minutes, lives, people, em, likes, things`\n",
    "- Brooklyn Nine-Nine: `T10 police, people, sounds, thinks, men, things, kids`\n",
    "- The Office US: `T00 people, people people, looks, turns, things, people things, things people` and `T01 knew, sales, things, people, smells, gotten, knew knew`\n",
    "\n",
    "Although a lot of the topics still had generic words such as 'people' and 'things', we can also see the specific words for each series such as 'parks', 'police', and 'sales'.\n",
    "\n",
    "### Word Embeddings (word2vec)\n",
    "\n",
    "For the word2vec model, I used the following important parameters:\n",
    "- window = 2\n",
    "- vector_size = 256\n",
    "- min_count = 50\n",
    "- workers = 4\n",
    "- OHCO = SCENES\n",
    "\n",
    "I only used nouns and verbs to generate the model, and I was able to generate an interesting scatter plot of words:\n",
    "\n",
    "![word2vec](pictures/word_1.png)\n",
    "\n",
    "This had interesting word clusters such as this group of nouns containing various reactions such as laughing, chuckling, grunts, and groans:\n",
    "\n",
    "![word2vec - Noun Group](pictures/word_2.png)\n",
    "\n",
    "Another interesting word cluster is with these verbs portaying different actions:\n",
    "\n",
    "![word2vec - Verb Group](pictures/word_3.png)\n",
    "\n",
    "Additionally, I checked for word analogies using this model. I wanted to see if this model can give insight on how genders are portrayed in workplace comedies. I tried the following analogies:\n",
    "\n",
    "![word2vec - Analogies](pictures/word_4.png)\n",
    "\n",
    "Interestingly, this seemed to represent the main characters of the shows. Both Brooklyn Nine-Nine (`police`) and The Office (`office, business`) mostly have male lead characters while Parks and Recreation (`department, government, parks`) has a female lead character. \n",
    "\n",
    "Additionally, I created separate wor2vec models for each TV series using the same parameters. They showed to have clusters with the main themes of the respective shows:\n",
    "\n",
    "Parks and Recreation (`parks, government, department`)\n",
    "![word2vec - Parks and Recreation](pictures/word_5.png)\n",
    "\n",
    "Brooklyn Nine-Nine (`undercover, precinct, detectives`)\n",
    "![word2vec - Brooklyn Nine-Nine](pictures/word_6.png)\n",
    "\n",
    "The Office US (`office, branch, manager`)\n",
    "![word2vec - The Office](pictures/word_7.png)\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "For sentiment analysis, I used the SALEX NRC lexicon. I created two models, using EPISODES and SEASONS as the OHCO levels.\n",
    "\n",
    "This is what part of the sentiment values for episodes look like:\n",
    "\n",
    "![Sentiment - Episodes](pictures/sentiment_1.png)\n",
    "\n",
    "The one for seasons was easier to visualize:\n",
    "\n",
    "![Sentiment - Seasons](pictures/sentiment_2.png)\n",
    "\n",
    "![Sentiment - Seasons, Bar](pictures/sentiment_3.png)\n",
    "\n",
    "It was interesting to see that seasons of Brooklyn Nine-Nine had generally lower sentiment values compared to the others.\n",
    "\n",
    "Additionally, I visualized the trends of the sentiments for each TV series. I used both seasons and episodes to do this.\n",
    "\n",
    "Parks and Recreation (Seasons):\n",
    "\n",
    "![PNR - Seasons](pictures/sentiment_4.png)\n",
    "\n",
    "Parks and Recreation (Episodes):\n",
    "\n",
    "![PNR - Episodes](pictures/sentiment_7.png)\n",
    "\n",
    "For Parks and Recreation, it is easier to see a smooth progression when comparing seasons instead of episodes.\n",
    "\n",
    "Brooklyn Nine-Nine (Seasons):\n",
    "\n",
    "![B99 - Seasons](pictures/sentiment_5.png)\n",
    "\n",
    "Brooklyn Nine-Nine (Episodes):\n",
    "\n",
    "![B99 - Seasons](pictures/sentiment_8.png)\n",
    "\n",
    "Similarly for Brooklyn Nine-Nine, the episode sentiment visualization was more erratic compared to the one for seasons.\n",
    "\n",
    "The Office (Seasons):\n",
    "\n",
    "![OFFICE - Seasons](pictures/sentiment_6.png)\n",
    "\n",
    "The Office (Episodes):\n",
    "\n",
    "![OFFICE - Episodes](pictures/sentiment_9.png)\n",
    "\n",
    "For The Office, it was interesting to see that there was an episode where all the emotion values peaked but it wasn't as evident when I visualized seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec17a70-e737-4eb8-8abf-de09effa7e17",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "This project was able to explore a lot of interesting things when it came to analyzing a corpus of sitcom scripts. Even using three different TV series combined in to one corpus showed promising results.\n",
    "\n",
    "For hierarchical clustering, we saw that consecutive seasons tend to be more related to each other, which would make sense since the storyline of the entire show continues to flow. There are also instances where some seasons are more separated from the rest of the show. One interesting example is the last two seasons of The Office (Season 8 and 9). The Office famously had main characters leave the show for its last few seasons. This change in character make-up could also play a role in this clustering. As discussed earlier, there are also cases where some seasons from other shows are more related to each other. Factors such as writing style could possibly play into this.\n",
    "\n",
    "For the principal components analysis, we saw that distinct words from the different shows tend to define the groups in the principal components. This makes sense since each TV series would have its own set of main themes. The same thing can be observed from the topic modeling and word embedding analyses that I conducted for this project, where distinct words from each series showed up in the respective models. It was a bit harder to generate the topic models, however, due to a lot of filler and generic words being used in the scripts. This can be attributed to the conversational nature of the language used in these scripts since sitcoms tend to portray day-to-day situations. \n",
    "\n",
    "For sentiment analysis, I was not able to see any distinct trends for the sentiments besides some sporadic dips and peaks for some episodes. This can show that the writers for each show tend to write episodes in the same way in terms of sentiment throughout all the seasons. \n",
    "\n",
    "Overall, I was able to see how these 3 famous workspace sitcoms (Parks and Recreation, Brooklyn Nine-Nine, The Office) were related to each other in terms of their episode scripts. Although there were some similarities observed, this exploration mostly showed how each show had their own unique distinction from the rest. This distinction and uniqueness would make sense as to why these 3 shows are beloved by their fans and viewers everywhere. I believe that this project can be used an exploratory framework for other similar projects. Maybe comparing a single-camera sitcom to a multi-camera sitcom would yield interesting results, or even comparing shows from drastically different decades or eras - there are a lot of exciting possibilities. This project has shown me how text analytics can provide valuable insight to various corpora. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61ab56-8b41-473d-8b75-6a8f6475e492",
   "metadata": {},
   "source": [
    "# Additional References\n",
    "\n",
    "[1] Charney, N. 2014. \"Cracking the Sitcom Code.\" The Atlantic. Retrieved May 1, 2023. https://www.theatlantic.com/entertainment/archive/2014/12/cracking-the-sitcom-code/384068/\n",
    "\n",
    "[2] MasterClass. 2022. \"How to Write a Sitcom: Sitcom Writing Guide.\" MasterClass. Retrieved May 1, 2023. https://www.masterclass.com/articles/how-to-write-a-sitcom\n",
    "\n",
    "[3] Miyamoto, K. 2023. \"Single-Camera vs Multi-Camera TV Sitcom Scripts: What's the Difference?\" Screen Craft. Retrieved May 1, 2023. https://screencraft.org/blog/differences-single-camera-multi-camera-tv-pilot-scripts/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e218820-3c34-4171-b6b5-7961fba9951a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
